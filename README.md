# ğŸ—½ NYC Restaurant Inspection Data Pipeline

This project is a pure Python data engineering pipeline that processes and analyzes New York City restaurant inspection data. The pipeline performs the following major steps:

- **Data Loading**
- **Data Cleaning**
- **Data Validation**
- **Data Transformation / Enrichment**
- **Data Storage to SQL Server**
- **SQL-Based Data Cleaning**
- **Automated Stored Procedure Execution**
- (Upcoming: Scheduling, Visualization, Reporting)

---

## ğŸ—ï¸ Project Architecture

```
nyc_data_project/
â”‚
â”œâ”€â”€ config/                   # Config files
â”‚   â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ data/                     # Raw CSV input files
â”‚
â”œâ”€â”€ logs/                     # Pipeline logs
â”‚   â””â”€â”€ pipeline.log
â”‚
â”œâ”€â”€ output/                   # Output CSVs (cleaned, transformed)
â”‚   â”œâ”€â”€ DOHMH_New_York_City_Restaurant_Inspection_C.csv
â”‚   â””â”€â”€ DOHMH_New_York_City_Restaurant_Inspection_T.csv
â”‚
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”œâ”€â”€ data_cleaner.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ data_transformer.py
â”‚   â”œâ”€â”€ data_validator.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ pipeline_runner.py
â”‚   â””â”€â”€ save_data.py
â”‚
â”‚â”€â”€ sql/
â”‚   â”œâ”€â”€ 01_create_raw_table.sql
â”‚   â”œâ”€â”€ 02_create_clean_table.sql
â”‚   â”œâ”€â”€ 03_cleaning_procedure.sql
â”‚ 
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
â””â”€â”€ README.md


```

---

## ğŸ§ª How to Run the Pipeline

1. **Install dependencies**  
```bash
pip install -r requirements.txt
```

2. **Update config.yaml**  
Make sure `csv_path` points to your raw CSV and `output_path` for saving files.

3. **Run the pipeline**  
```bash
python src/pipeline_runner.py
```

4. **(Optional)**: View cleaned data  
```bash
python view_cleaned_data.py
```

---

## âœ… Completed Modules

- [x] Data Loader
- [x] Data Cleaner
- [x] Data Validator
- [x] Data Transformer / Enricher
- [x] Logging System (modular, prefixed, and auto-truncated)
- [x] SQL Server Integration (Upload, Truncate)
- [x] Stored Procedure Execution from Python
- [x] SQL Server Post-Processing (cleaning into new table)
- [x] Git + GitHub setup
- [x] Power BI Dashboard (Visualizations from SQL Server)
- [x] Apache Airflow Orchestration (DAG to automate the pipeline)

---

## ğŸ“Š Dashboard & Visualizations

Power BI is used to visualize the cleaned and enriched data stored in SQL Server.

**Key visuals include:**
- ğŸ“… Inspection Score Trends by Year and Borough
- ğŸ—ºï¸ Risk Category Breakdown by Borough and Cuisine
- ğŸ“Œ Top 10 Restaurants by Violation Count
- ğŸ“ˆ Grade Distribution Across Time
- ğŸ§¹ Common Violation Codes by Cuisine
- ğŸ” Slicers for Borough, Grade, Year, Cuisine

> The `.pbix` file is available under `/powerbi/` folder.  
> Manual refresh is required for now since Power BI Service is not connected.

---

## ğŸ“¦ Airflow DAG Orchestration

An Apache Airflow DAG is created to automate the end-to-end data pipeline.

**DAG Features:**
- Load CSV â†’ Clean â†’ Validate â†’ Transform
- Upload to SQL Server
- Execute stored procedure for database cleaning
- Config-driven architecture (via YAML)

Airflow runs under WSL2 using Ubuntu with the DAG file located in: airflow_dags/nyc_inspection_dag.py


> Run: `airflow webserver` + `airflow scheduler` to start orchestration.


---

## ğŸ“’ Notes

- All logs are saved in logs/pipeline.log and refreshed each time the pipeline is run.
- The database cleaning logic lives inside the SQL Server stored procedure

---

## ğŸ“Œ Author

**Sree Madhuchandra Gamidi**  
GitHub: [@Madhugamidi14](https://github.com/Madhugamidi14)
